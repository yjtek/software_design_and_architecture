{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caches can help you save money, simply be storing results from frequent queries / computationally heavy queries where it is easily looked up!\n",
    "    - i.e. check if the question has been asked before, if it has, just return whatever you answered previously\n",
    "\n",
    "- DBs can deal with uneven loads of reads/writes by time. With a cache, you prevent spikes in DB hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Client caching\n",
    "    - Store a copy of the response on the client side (i.e. in the user's browser/device)\n",
    "\n",
    "- CDN caching\n",
    "    - CDNs can store a copy of the result for an entire segment of users\n",
    "\n",
    "- Web server caching\n",
    "    - Reverse proxies can cache results of frequent queries\n",
    "\n",
    "- Database caching\n",
    "    - DB implements caching to avoid computational hits\n",
    "\n",
    "- Application caching\n",
    "    - In-memory caches like Redis/Memcached can sit between your application and DB\n",
    "    - Data is stored in RAM, so it is fast\n",
    "    - But RAM is limited\n",
    "    - So you need a decent cache invalidation strategy. Typical strategy is to drop least recently used (LRU) entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granularity of Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query Level\n",
    "    - When a query hits your DB, hash the query and cache the result\n",
    "    - BUT downsides are:\n",
    "        - Hard to delete a cached result with complex queries, unless you preset an expiration time\n",
    "        - If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell\n",
    "\n",
    "- Object level\n",
    "    - Your query result is an object\n",
    "    - Hash it and cache it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Space is always limited, so you can't cache everything. \n",
    "- So you need to choose a cache update strategy from below\n",
    "\n",
    "- Cache-aside\n",
    "    - What is it?\n",
    "        - Application reads and writes from storage, and is in charge of caching. Cache does not interact with DB\n",
    "        - Steps:\n",
    "            - Application will look for entry in cache.\n",
    "            - If cache miss, look up DB\n",
    "            - Add entry to cache\n",
    "            - Return\n",
    "    - Disadvanges\n",
    "        - Each cache miss takes three trips (get result from DB, compute, and add to cache), which can cause a noticeable delay.\n",
    "        - Cached data can become stale if DB is updated. Mitigate by setting a time-to-live (TTL) \n",
    "        - When a node fails, it is replaced by a new, empty node, so everything has to be recached.\n",
    "\n",
    "- Write-through\n",
    "    - What is it?\n",
    "        - Cache is used as the main DB, so you always read from and write to the cache. The cache then updates the DB\n",
    "        - This way, always guarantee that cache data is not stale\n",
    "        - Users generally are more ok with latency when writing data than when reading\n",
    "    - Disadvanges\n",
    "        - This is obviously slow because you need to write to 2 places, but subsequent reads are fast. \n",
    "        - If a node fails, the new node will not have cache entries until the entry is updated in the DB. \n",
    "        - Wasted space in cache because most data written might never be read\n",
    "\n",
    "- Write-behind (write-back)\n",
    "    - What is it?\n",
    "        - Add/update entry in cache\n",
    "        - Asynchronously write entry to the data store, improving write performance because you don't need to wait until the DB write finishes before returning to user\n",
    "    - Disadvanges\n",
    "        - If you cache dies before the async write is complete, you can lose data\n",
    "        - More complex to implement write-behind than it is to implement cache-aside or write-through.\n",
    "\n",
    "- Refresh-ahead\n",
    "    - What is it?\n",
    "        - Cache automatically refreshes any recently accessed cache entry prior to its expiration.\n",
    "        - Can result in reduced latency vs read-through if the recently accessed cache entries are predictive of future needs \n",
    "\n",
    "    - Disadvanges\n",
    "        - If past use is not predictive of future use, then you may be storing a lot of junk in cache for nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General disadvantages of caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Need to maintain consistency between caches and the source of truth such as the database through cache invalidation.\n",
    "- Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.\n",
    "- Need to make application changes such as adding Redis or memcached."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
