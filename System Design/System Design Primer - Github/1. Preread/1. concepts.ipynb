{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.youtube.com/watch?v=-W9F__D3oY4\n",
    "- https://web.archive.org/web/20221030091841/http://www.lecloud.net/tagged/scalability/chrono"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vertical/Horizontal Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vertical scaling: Get a better machine with more RAM, processors, disk space etc\n",
    "    - But there is a limit obviously, you are always going to be capped at some point. Either you run out of money, or you run into the tech frontier\n",
    "\n",
    "- Horizontal scaling: Add MORE stuff (multiple databases, multiple compute instances etc)\n",
    "    - Instead of getting 1 super good machine, get 99 lousy ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have multiple web servers (horizontal scaling), we need a way to distribute traffic\n",
    "- Hence, load balancer. Expose the public IP of the load balancer, and switch all servers to private IP\n",
    "- One way to load balance; round robin (BIND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shared Session State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But imagine, as a user, if I'm routed to server 1, and my work gets cached as some session\n",
    "- The next time I look up the resource, if I go to server 2, I may lose my work, because sessions are stored within the machine\n",
    "- So to avoid this; we can simply factor out the \"session\" state that is connected to all servers. Now, shared session data exists across all servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But now that shared state is stored in one place, it is a single point of failure\n",
    "- Instead of storing on harddisk, we can use RAID (redundant array of independent disks)\n",
    "- Idea\n",
    "    - You have 2 harddrives, and you stripe data across them\n",
    "    - Every time your OS wants to save a file, you alternate writing a bit to each drive Which effectively doubles the rate at which I can write my file\n",
    "    - Alternatively, I mirror from 1 disk to the other\n",
    "\n",
    "- So for a given amount of space, some portion is sacrificed for redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Shared Storage Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shared storage solutions include network attached storage (NAS), storage area networks (SAN), and redundant arrays of independent disks (RAID), as well as dedicated storage servers and cloud storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Database Replication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To avoid losing information, you may also wish to replicate your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Session Affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you enable session affinity, your load balancer directs all requests from a particular end user to a specific origin server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. In-memory Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Store data in computer's RAM instead of writing to disk\n",
    "- Faster access, but may encounter cold start issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Replication - Active/Passive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Active\n",
    "    - Process the same request in every replica\n",
    "\n",
    "- Passive\n",
    "    - copies the contents from one database, known as the master database, to the other, known as the replica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Break data into specified groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Data Center Redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Self explanatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Scalability for dummies: Clones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Public servers of a scalable web service are hidden behind a load balancer, which distributes user requests onto your application servers. \n",
    "    - e.g. If user 1 interacts with your service, he may be served at his first request by server 2, then with his second request by server 9 etc.\n",
    "\n",
    "- Ideally, user should always get the same results of his request back regardless of what server he \"landed on\". \n",
    "    - First golden rule for scalability: every server contains exactly the same codebase and does not store any unique data\n",
    "\n",
    "- What about user sessions? Shouldn't these be unique?\n",
    "    - User sessions should be stored in a centralized data store accessible to all application servers. \n",
    "    - Either an external database or an external persistent cache, like Redis. \n",
    "    - Redis will have better performance than a database. \n",
    "    - External just means the data store does not reside on the application servers\n",
    "\n",
    "- How can I do a deployment with multiple servers, to avoid having some servers serving old code, and some serving new? \n",
    "    - Orchestration of deployment is a solved issue\n",
    "\n",
    "- Finally, create an image file from one of these servers (AWS calls this AMI - Amazon Machine Image). \n",
    "    - Use this AMI as a “super-clone” that all your new instances are based upon. \n",
    "    - Whenever you start a new instance/clone, just do an initial deployment of your latest code and you are ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Scalability for dummies: Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now your servers can now horizontally scale and serve thousands of concurrent requests. But somewhere down the line, your application slows and breaks due to your DB, which is probably MySQL\n",
    "\n",
    "- How can we scale our DB? 2 paths:\n",
    "    1. Stick with MySQL and keep it running. Hire a database administrator (DBA), ask him to do master-slave replication (read from slaves, write to master) and upgrade your master server by adding RAM. \n",
    "        - This is not scalable. At some point you will start hunting for ways to optimise your DB server even more (sharding, denormalisation, SQL tuning)\n",
    "        - Then it'll reach a point where keeping your DB running is going to cost you an obscene amount of $\n",
    "        \n",
    "    2. Denormalise right from the beginning, and include no more Joins in any database query.\n",
    "        - Stay in MySQL and use it like a NoSQL database\n",
    "        - Now you need to do joins in your business logic\n",
    "        - But the sooner you do this, the less code you will have to change in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Scalability for dummies: Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the above we can still end up with a slow DB. Even though the server side is fine, the user experience may be bad due to DB loan\n",
    "\n",
    "- To improve performance, use an in-memory cache like `Memcached` or `Redis`, so repeated queries can simply be returned without computation\n",
    "\n",
    "- 2 patterns of caching data\n",
    "    - Cached DB queries, where you store the result data in cache, with the hashed query as the key\n",
    "        - However, caches are not free, and you need to be aware of when the cache can be invalidated\n",
    "\n",
    "    - Cached objects. Instead of caching only the data, cache the entire object fetching and constructing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Scalability for dummies: Asynchronism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example:\n",
    "\n",
    "    - A user comes to your website and starts a very computing intensive task which would take several minutes to finish. \n",
    "    - frontend sends a job to a job queue and immediately signals back to the user: your job is in work, please continue to the browse the page. The job queue is constantly checked by a bunch of workers for new jobs. If there is a new job then the worker does the job and after some minutes sends a signal that the job was done. The frontend, which constantly checks for new “job is done” - signals, sees that the job was done and informs the user about it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
